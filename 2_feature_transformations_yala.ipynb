{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q phonenumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.types import DecimalType\n",
    "\n",
    "# Snowpark ML\n",
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.metrics.correlation import correlation\n",
    "\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.registry import model_registry\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Data Science Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Misc\n",
    "import json\n",
    "import joblib\n",
    "import cachetools\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import phonenumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Snowpark Connection\n",
    "\n",
    "\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the table name where we stored the diamonds dataset\n",
    "# **nChange this only if you named your table something else in the data ingest notebook **\n",
    "CONVERTEDONLY_LEADS_TABLE = 'CONVERTEDONLY'\n",
    "input_tbl = f\"{session.get_current_database()}.{session.get_current_schema()}.{CONVERTEDONLY_LEADS_TABLE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n",
    "leads_df = session.table(input_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df = leads_df.drop('SALUTATION'\n",
    ",'TITLE'\n",
    ",'FIRSTNAME'\n",
    ",'LASTNAME'\n",
    ",'WEBSITE'\n",
    ",'CONVERTEDCONTACTID'\n",
    ",'CONVERTEDDATE'\n",
    ",'CONVERTEDCONTACTID'\n",
    ",'CONVERTEDORGANIZATIONID'\n",
    ",'UPDATEDDATE'\n",
    ",'EMPLOYEECOUNT'\n",
    ",'LEADRATING'\n",
    ",'FAX'\n",
    ",'INDUSTRY'\n",
    ",'OWNERUSERID'\n",
    ",'RESPONSIBLEUSERID'\n",
    ",'ADDRESSSTREET'\n",
    ",'ADDRESSCITY'\n",
    ",'ADDRESSSTATE'\n",
    ",'ADDRESSPOSTCODE'\n",
    ",'LASTACTIVITYDATE'\n",
    ",'NEXTACTIVITYDATE'\n",
    ",'VISIBLETO'\n",
    ",'VISIBLETEAMID'\n",
    ",'ORGANIZATIONNAME'\n",
    ",'CREATEDUSERID'\n",
    ",'IMAGEURL'\n",
    ",'TAGS'\n",
    ",'O_ID'\n",
    ", 'CONVERTED'\n",
    ", 'CONVERTEDOPPORTUNITYID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the leads_df EMAIL dataframe column\n",
    "leads_df = leads_df.na.replace('', \"unknown@unknown.nl\", subset='EMAIL')\n",
    "# leads_df.write.mode('overwrite').save_as_table('tmp_leadsonly')\n",
    "leads_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of identified private email domains\n",
    "private_domains = [\n",
    "    'gmail.com', 'hotmail.com', 'orange.fr', 'wanadoo.fr', 'hotmail.fr',\n",
    "    'yahoo.com', 'outlook.com', 'hotmail.co.uk', 'me.com', 'gmx.de',\n",
    "    'icloud.com', 'yahoo.it', 'libero.it', 'web.de', 'kpnmail.nl',\n",
    "    'yahoo.de', 'yahoo.fr', 'free.fr', 'telenet.be', 'live.fr',\n",
    "    'otenet.gr', 'mac.com', 'yahoo.co.uk', 'laposte.net', 'uol.com.br',\n",
    "    'casema.nl', 'aol.com', 't-online.de', 'unknown.com', 'yahoo.in',\n",
    "    'gmx.fr', 'mail.com', 'mail.ru', 'live.it', 'msn.com',\n",
    "    'yahoo.com.sg', 'hotmail.it', 'googlemail.com', 'hotmail.nl', 'ziggo.nl'\n",
    "]\n",
    "\n",
    "# Extracting the domain from the 'Email' column\n",
    "leads_df = leads_df.withColumn(\n",
    "    \"Email_Domain\",\n",
    "    F.regexp_extract(F.col(\"Email\"), '@([a-zA-Z0-9.-]+)$', 1)\n",
    ")\n",
    "\n",
    "# leads_df.write.mode('overwrite').save_as_table('tmp_leadsonly')\n",
    "# leads_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying the email addresses as 'Private' or 'Business'\n",
    "# merged_data_prefixed['c_Email_Type'] = merged_data_prefixed['c_Email_Domain'].apply(\n",
    "#     lambda x: 'Private' if x in private_domains else 'Business')\n",
    "\n",
    "# Creating a column to classify email as 'Private' or 'Business'\n",
    "leads_df = leads_df.withColumn(\n",
    "    \"Email_Type\",\n",
    "    F.when(F.col(\"Email_Domain\").isin(private_domains), \"Private\").otherwise(\"Business\")\n",
    ")\n",
    "leads_df = leads_df.drop('EMAIL_DOMAIN')\n",
    "# leads_df.write.mode('overwrite').save_as_table('tmp_leadsonly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'LENGTH_LEADDESCRIPTION' indicating the length of the text in 'LEADDESCRIPTION'\n",
    "leads_df = leads_df.withColumn(\"LENGTH_LEADDESCRIPTION\", F.length(F.col(\"LEADDESCRIPTION\")))\n",
    "leads_df = leads_df.drop('LEADDESCRIPTION')\n",
    "# leads_df.write.mode('overwrite').save_as_table('tmp_leadsonly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the LEN_LEADDESC_NORM column\n",
    "snowml_mms = snowml.MinMaxScaler(input_cols=[\"LENGTH_LEADDESCRIPTION\"], output_cols=[\"LEN_LEADDESC_NORM\"])\n",
    "leads_df = snowml_mms.fit(leads_df).transform(leads_df)\n",
    "\n",
    "# Reduce the number of decimals\n",
    "new_col = leads_df.col(\"LEN_LEADDESC_NORM\").cast(DecimalType(7, 6))\n",
    "leads_df = leads_df.with_column(\"LEN_LEADDESC_NORM\", new_col)\n",
    "\n",
    "leads_df = leads_df.drop('LENGTH_LEADDESCRIPTION')\n",
    "# leads_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'PHONENUMBER' based on 'MOBILE' and 'PHONE'\n",
    "leads_df = leads_df.withColumn(\n",
    "    \"PHONENUMBER\",\n",
    "    F.when(F.col(\"MOBILE\").isNull() | (F.col(\"MOBILE\") == ''), F.col(\"PHONE\")).otherwise(F.col(\"MOBILE\"))\n",
    ")\n",
    "# leads_df.write.mode('overwrite').save_as_table('TMP_LEADSONLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'COUNTRYCODE' based on 'PHONENUMBER'\n",
    "\n",
    "def get_country_code(phone_number):\n",
    "    # return phn.parse(phone_number, None)\n",
    "    try:\n",
    "        parsed_number = phonenumbers.parse(phone_number, None)\n",
    "        if phonenumbers.is_valid_number(parsed_number):\n",
    "            return '+' + str(parsed_number.country_code)\n",
    "    except:\n",
    "        pass\n",
    "    return '+00'\n",
    "\n",
    "# Assuming leads_df is your Snowflake DataFrame\n",
    "leads_df_pandas = leads_df.to_pandas()\n",
    "\n",
    "leads_df_pandas['COUNTRYCODE'] = leads_df_pandas['PHONENUMBER'].apply(\n",
    "    lambda x: get_country_code(str(x)) if pd.notna(x) else '+00'\n",
    ")\n",
    "\n",
    "session.write_pandas(leads_df_pandas, \"TEMP_LEADS_TABLE\", auto_create_table=True)\n",
    "\n",
    "# Read the data back into a Snowflake DataFrame\n",
    "leads_df_snowflake = session.table(\"TEMP_LEADS_TABLE\")\n",
    "\n",
    "# Write the Snowflake DataFrame to a permanent table\n",
    "# leads_df_snowflake.write.mode(\"overwrite\").save_as_table(\"TMP_LEADSONLY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the leads_df EMAIL dataframe column\n",
    "leads_df_snowflake = leads_df_snowflake.na.replace('', \"Netherlands\", subset='ADDRESSCOUNTRY')\n",
    "leads_df_snowflake = leads_df_snowflake.na.replace('-', \"Netherlands\", subset='ADDRESSCOUNTRY')\n",
    "# leads_df_snowflake.write.mode('overwrite').save_as_table('TMP_LEADSONLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting 'CreatedDate' into Year, Month, Week, Day, and Hour portions with specified format\n",
    "\n",
    "# Assuming leads_df is your Snowflake DataFrame and 'CreatedDate' is a column in this DataFrame\n",
    "\n",
    "# Extracting year, month, week, day, and hour from 'CreatedDate'\n",
    "# Convert 'CreatedDate' to a timestamp type with the specified format\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedDate', F.to_timestamp('CreatedDate', 'MM/DD/YYYY HH12:MI:SS AM'))\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedYear', F.year('CreatedDate'))\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedMonth', F.month('CreatedDate'))\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedWeek', F.weekofyear('CreatedDate'))\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedDay', F.dayofweek('CreatedDate'))  # Note: In Snowflake, Sunday=0, Saturday=6\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedHour', F.hour('CreatedDate'))\n",
    "\n",
    "# Adjust 'CreatedDay' to make Monday=1, Sunday=7 if needed\n",
    "leads_df_snowflake = leads_df_snowflake.withColumn('CreatedDay', (F.col('CreatedDay') % 7) + 1)\n",
    "# leads_df_snowflake.write.mode('overwrite').save_as_table('tmp_leadsonly')\n",
    "# leads_df_snowflake.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df_snowflake = leads_df_snowflake.drop('MOBILE', 'PHONE', 'PHONENUMBER', 'LEADSTATUSID', 'EMAIL', 'COUNTRYCODE', 'CUSTOMFIELDS', 'CREATEDYEAR', 'CREATEDDATE')\n",
    "# leads_df_snowflake.write.mode('overwrite').save_as_table('TMP_LEADSONLY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df_snowflake.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals to numeric columns\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=[\"LEADSOURCEID\", \"ADDRESSCOUNTRY\", \"EMAIL_TYPE\"], output_cols=[\"LS_\", \"AD_\", \"ET_\"])\n",
    "# transformed_leads_df = snowml_ohe.fit(leads_df_snowflake).transform(leads_df_snowflake)\n",
    "leads_df_snowflake = snowml_ohe.fit(leads_df_snowflake).transform(leads_df_snowflake)\n",
    "\n",
    "np.array(leads_df_snowflake.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df_snowflake = leads_df_snowflake.drop('ID', 'LEADSOURCEID', 'ADDRESSCOUNTRY', 'EMAIL_TYPE')\n",
    "np.array(leads_df_snowflake.columns)\n",
    "# transformed_leads_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize all the features for modeling\n",
    "\n",
    "# Categorize all the features for processing\n",
    "# CATEGORICAL_COLUMNS = [\"CUT\", \"COLOR\", \"CLARITY\"]\n",
    "# CATEGORICAL_COLUMNS_OE = [\"CUT_OE\", \"COLOR_OE\", \"CLARITY_OE\"] # To name the ordinal encoded columns\n",
    "# CATEGORICAL_COLUMNS_OHE = 'auto',\n",
    "NUMERICAL_COLUMNS = [\"CREATEDMONTH\", \"CREATEDWEEK\", \"CREATEDHOUR\", \"CREATEDDAY\", \"LEN_LEADDESC_NORM\"]\n",
    "ONE_HOT_ENCODED_COLUMNS = [\"LEADSOURCEID\", \"ADDRESSCOUNTRY\", \"EMAIL_TYPE\"]  # New list for one-hot encoded columns\n",
    "# ONE_HOT_ENCODED_COLUMNS_OUTPUT = [\"LS\", \"AC\", \"ET\"]\n",
    "ONE_HOT_ENCODED_COLUMNS_OUTPUT = [\"OHE\", \"OHE\", \"OHE\"]\n",
    "\n",
    "LABEL_COLUMNS = ['O_OPPORTUNITYSTATE']\n",
    "OUTPUT_COLUMNS = ['PREDICTED_OPPORTUNITYSTATE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "leads_df_snowflake_train, leads_df_snowflake_test = leads_df_snowflake.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "# Run the train and test sets through the Pipeline object we defined earlier\n",
    "# train_df = preprocessing_pipeline.fit(leads_df_snowflake_train).transform(leads_df_snowflake_train)\n",
    "# test_df = preprocessing_pipeline.transform(leads_df_snowflake_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(leads_df_snowflake_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train_df' is the transformed training dataframe\n",
    "one_hot_encoded_columns = [col for col in leads_df_snowflake_train.columns \n",
    "                           if col.startswith(('LS__', '\"AD__', '\"ET__'))]\n",
    "\n",
    "one_hot_encoded_columns = [col.replace('\"', '') for col in one_hot_encoded_columns]\n",
    "\n",
    "# one_hot_encoded_columns = [col for col in leads_df_snowflake_train.columns if 'OHE_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(leads_df_snowflake_test))\n",
    "print(type(leads_df_snowflake_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# classifier.fit(train_df[feature_columns], train_df[target_column])\n",
    "# classifier.fit(leads_df_snowflake_train[feature_columns], leads_df_snowflake_train[target_column])\n",
    "feature_columns = NUMERICAL_COLUMNS + one_hot_encoded_columns\n",
    "\n",
    "leads_df_snowflake_train = leads_df_snowflake_train.to_pandas()\n",
    "leads_df_snowflake_test = leads_df_snowflake_test.to_pandas()\n",
    "\n",
    "X_train = leads_df_snowflake_train[feature_columns]\n",
    "y_train = leads_df_snowflake_train['O_OPPORTUNITYSTATE']\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Now fit the model\n",
    "# XGBClassifier.fit(leads_df_snowflake_train[feature_columns], leads_df_snowflake_train['O_OPPORTUNITYSTATE'])\n",
    "\n",
    "# Predict\n",
    "# predictions = classifier.predict(leads_df_snowflake_test[feature_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming leads_df_snowflake_test is a Pandas DataFrame\n",
    "leads_df_snowflake_test.columns = [col.replace('\"', '') for col in leads_df_snowflake_test.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_classifier.predict(leads_df_snowflake_test[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = xgb_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_columns, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'leads_df_snowflake_test' is your test dataset\n",
    "# and 'O_OPPORTUNITYSTATE' is the column with the true labels\n",
    "\n",
    "y_test = leads_df_snowflake_test['O_OPPORTUNITYSTATE']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Display metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have predictions and true labels\n",
    "y_true = y_test  # Replace with your actual true labels\n",
    "y_pred = predictions  # Replace with your model's predictions\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot using seaborn\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a SHAP explainer object\n",
    "explainer = shap.Explainer(xgb_classifier)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(leads_df_snowflake_test[feature_columns])\n",
    "\n",
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, leads_df_snowflake_test[feature_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet = leads_df_snowflake_test.head(5)\n",
    "# print(snippet.to_csv(index=False))\n",
    "print(snippet.to_json(orient='records', lines=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific instance (e.g., the first instance in your test set)\n",
    "instance_to_explain = leads_df_snowflake_test[feature_columns].iloc[0]\n",
    "\n",
    "# Calculate SHAP values for this instance\n",
    "shap_values_instance = explainer.shap_values(instance_to_explain)\n",
    "\n",
    "# Visualize the first prediction's explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values_instance, instance_to_explain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-ml-hol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
